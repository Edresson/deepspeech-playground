{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# use CPU or GPU\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['THEANO_FLAGS'] = 'device=cuda0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:02:00.0)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and weight loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from random import Random\n",
    "import json\n",
    "\n",
    "rng = Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import beamsearch\n",
    "reload(beamsearch)\n",
    "from utils import argmax_decode, word_error_rate, for_tf_or_th\n",
    "from beamsearch import beam_decode, beam_decode_u\n",
    "\n",
    "lm = kenlm.Model('data/lm/lm.binary')\n",
    "\n",
    "def iterate_weights(model_path):\n",
    "    \"\"\"Iterate over saved model weights\"\"\"\n",
    "    for model_weight in glob(os.path.join(model_path, '') + '*.h5'):\n",
    "        yield model_weight\n",
    "\n",
    "def pick_sample_files(desc_file, count, min_duration, max_duration):\n",
    "    metadata = []\n",
    "    with open(desc_file) as f:\n",
    "        for line in f:\n",
    "            metadata.append(json.loads(line))\n",
    "    legitimates = [ sample for sample in metadata if min_duration <= sample['duration'] <= max_duration ]\n",
    "    rng.shuffle(legitimates)\n",
    "    return legitimates[:count]\n",
    "\n",
    "def test_generator(datagen, test_samples, batch_size=64, normalize=True):\n",
    "    global in_\n",
    "    texts = [s['text'] for s in test_samples]\n",
    "    durations = [s['duration'] for s in test_samples]\n",
    "    paths = [s['key'] for s in test_samples]\n",
    "    features = [datagen.featurize(p) for p in paths]\n",
    "    if normalize:\n",
    "        features = [datagen.normalize(f) for f in features]\n",
    "\n",
    "    for i in range( np.ceil(len(features) / float(batch_size)).astype(int) ):\n",
    "        batch_durations = durations[i*batch_size: (i+1)*batch_size]\n",
    "        batch_features = features[i*batch_size: (i+1)*batch_size]\n",
    "        batch_texts = texts[i*batch_size: (i+1)*batch_size]\n",
    "        batch_paths = paths[i*batch_size: (i+1)*batch_size]\n",
    "        max_length = max([f.shape[0] for f in batch_features])\n",
    "        batch_array = np.zeros((len(batch_features), max_length, features[0].shape[1]), dtype='float32')\n",
    "        for fi in range(len(batch_features)):\n",
    "            batch_array[fi, :batch_features[fi].shape[0], :] = batch_features[fi]\n",
    "        yield {'x': batch_array, 'y': batch_texts, 'path': batch_paths, 'duration': batch_durations}\n",
    "\n",
    "def best_lm_alternative(true_sentence, wer, predictions, verbose=False):\n",
    "    \"\"\" predictions is a list of tuples which first denote sentence and next is It's probablity\n",
    "    \"\"\"\n",
    "    best, best_score = None, np.finfo('float32').min\n",
    "    for s, p in predictions:\n",
    "        lm_score = lm.score(s)\n",
    "        if lm_score > best_score:\n",
    "            best, best_score = s, lm_score\n",
    "    if best == predictions[0][0]:\n",
    "        if verbose:\n",
    "            print \"language model didn't change prediction\"\n",
    "        best_wer = wer\n",
    "    else:\n",
    "        best_wer = word_error_rate([true_sentence], [best], decoded=True)[0]\n",
    "        if verbose:\n",
    "            print \"language model changed prediction, WER changed from {old_wer} to {new_wer}\".format(\n",
    "                old_wer = wer, new_wer = best_wer\n",
    "            )\n",
    "    return best, best_wer\n",
    "\n",
    "def evaluate(batch_generator, output_fn, learning_phase=False, use_lm=False, beam_width=12):\n",
    "    all_nolm_wers, all_lm_wers = [], []\n",
    "    for batch in batch_generator:\n",
    "        net_out = output_fn([batch['x'], learning_phase])[0]\n",
    "        mtp_net_out = for_tf_or_th(net_out, net_out.swapaxes(0, 1))\n",
    "        pred_texts = [argmax_decode(o) for o in mtp_net_out]\n",
    "        nolm_wers = word_error_rate(batch['y'], pred_texts, True)\n",
    "        all_nolm_wers.append(nolm_wers)\n",
    "        \n",
    "        if use_lm:\n",
    "            alt_beam_preds = lambda i: zip(*beam_decode_u(mtp_net_out[i, :, :], beam_width, normalize=True))\n",
    "            pred_texts, lm_wers = zip(*[best_lm_alternative(batch['y'][i], nolm_wers[i], alt_beam_preds(i))\n",
    "                                      for i in range(mtp_net_out.shape[0])])\n",
    "            all_lm_wers.append(np.array(lm_wers))\n",
    "            all_wers = all_lm_wers\n",
    "        else:\n",
    "            all_wers = all_nolm_wers\n",
    "        \n",
    "        for i, y in enumerate(batch['y']):\n",
    "            print 'r:{}\\np:{}\\n{}: WER: {}, DURATION: {}, PATH: {}'.format(y, pred_texts[i], i, all_wers[-1][i], batch['duration'][i], batch['path'][i])\n",
    "        print 'batch mean WER: {}'.format(all_wers[-1].mean())\n",
    "    if use_lm:\n",
    "        print 'LM WER: {} No LM WER: {}'.format(np.concatenate(all_lm_wers).mean(), np.concatenate(all_nolm_wers).mean())\n",
    "    else:\n",
    "        'whole mean WER: {}'.format(np.concatenate(all_wers).mean())\n",
    "    return mtp_net_out, pred_texts, all_wers, batch['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_desc = '/home/reith/deepspeech/ba-dls-deepspeech/descs/test-clean.json'\n",
    "#test_desc = '/home/reith/deepspeech/ba-dls-deepspeech/descs/test-other.json'\n",
    "#test_desc = '/home/reith/deepspeech/ba-dls-deepspeech/descs/dev-clean.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generator import DataGenerator\n",
    "datagen = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_samples = pick_sample_files(test_desc, 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize by input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_desc = '/home/reith/deepspeech/ba-dls-deepspeech/descs/train-clean-360.json'\n",
    "datagen.load_train_data(train_desc, 15)\n",
    "datagen.fit_train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen.reload_norm('860-1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and test weights of a half-phoneme model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_dir = '/home/reith/deepspeech/ba-dls-deepspeech/models/22-cont-23-i9696-lr1e-4-train-360-dur15/'\n",
    "#model_dir = '/home/reith/deepspeech/ba-dls-deepspeech/models/23-cont-i2494-joingrus-dur15-nobn-lr5e-5/'\n",
    "model_dir = '/home/reith/deepspeech/ba-dls-deepspeech/models/24-cont-train-860'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of training procedure:\n",
    "- 7 Epochs of dual phoneme-text on train-100 (20)\n",
    "- 3 Epochs on train-500 for phoenme fine-tuning (21)\n",
    "- 3 Epochs on train-500 for text fine-tuning (22)\n",
    "- 2 Epochs on train-360 (23)\n",
    "- 2 Epochs on train-360 dropping phoneme branch and and batch normalization (24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make half phoneme model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:model_wrp:ignoring unsuported batchnorm mode of 2 on keras 2\n",
      "WARNING:model_wrp:ignoring unsuported batchnorm mode of 2 on keras 2\n",
      "WARNING:model_wrp:ignoring unsuported batchnorm mode of 2 on keras 2\n",
      "WARNING:model_wrp:ignoring unsuported batchnorm mode of 2 on keras 2\n",
      "WARNING:model_wrp:ignoring unsuported batchnorm mode of 2 on keras 2\n",
      "model_wrp.py:374: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[Reshape{3..., inputs=/acoustic_...)`\n",
      "  self.model = Model(input=acoustic_input, output=[phoneme_out, text_out])\n"
     ]
    }
   ],
   "source": [
    "from model_wrp import HalfPhonemeModelWrapper\n",
    "model_wrp = HalfPhonemeModelWrapper()\n",
    "model = model_wrp.compile(nodes=1000, conv_context=5, recur_layers=5)\n",
    "output_fn = model_wrp.compile_output_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or gru model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_wrp.py:274: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(29, activation=\"linear\", kernel_initializer=\"glorot_uniform\", name=\"text_dense\")`\n",
      "  activation=for_tf_or_th('softmax', 'linear')\n",
      "model_wrp.py:278: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[Reshape{3..., inputs=/acoustic_...)`\n",
      "  self.model = Model(input=acoustic_input, output=[network_output])\n"
     ]
    }
   ],
   "source": [
    "from model_wrp import GruModel\n",
    "model_wrp = GruModel()\n",
    "model = model_wrp.compile(nodes=1000, conv_context=5, recur_layers=5, batch_norm=False)\n",
    "output_fn = model_wrp.compile_output_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(os.path.join(model_dir, 'best-val-weights.h5'))\n",
    "model.load_weights(os.path.join(model_dir, 'model_19336_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of training procedure:\n",
    "- 3 Epochs of dual phoneme-text on train-100 by dropout of 0.3 and leaky relu factor of 0.05 (40)\n",
    "- 5 Epochs on train-100 for phoenme fine-tuning (41)\n",
    "- 5 Epochs on train-100 for text fine-tuning (42)\n",
    "- 5 Epochs on train-360 (43)\n",
    "- 5 Epochs on train-860 dropping phoneme branch and and batch normalization and reduced dropout to 0.1 (44)\n",
    "- 20 Epochs on train-860 reduced learning rate down to 5e-5 and for samples up to 20 seconds long (45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/home/reith/deepspeech/ba-dls-deepspeech/models/44-cont-45-i14490-dur20-lr5e-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_wrp.py:274: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(29, activation=\"softmax\", kernel_initializer=\"glorot_uniform\", name=\"text_dense\")`\n",
      "  activation=for_tf_or_th('softmax', 'linear')\n",
      "model_wrp.py:278: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"ac...)`\n",
      "  self.model = Model(input=acoustic_input, output=[network_output])\n"
     ]
    }
   ],
   "source": [
    "from model_wrp import GruModel\n",
    "model_wrp = GruModel()\n",
    "model = model_wrp.compile(nodes=1000, conv_context=5, recur_layers=5, dropout=.1, lirelu_alpha=.05, batch_norm=False)\n",
    "output_fn = model_wrp.compile_output_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_dir, 'best-val-weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = evaluate(test_generator(datagen, test_samples, normalize=True), output_fn, use_lm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate(test_generator(datagen, test_samples, normalize=True), output_fn, beam_width=27, use_lm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swam']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thus idleness is the mother\n",
    "# thus i don't lissisthe mother\n",
    "def edits(word):\n",
    "    letters = ''.join([chr(i) for i in range(ord('a'), ord('z') + 1)])\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [l + r[1:] for l, r in splits if r]\n",
    "    transposes = [l + r[1] + r[0] + r[2:] for l, r in splits if len(r) >1]\n",
    "    replaces = [l + c + r[1:] for c in letters for l, r in splits if r]\n",
    "    inserts = [l + c + r for c in letters for l, r in splits if r]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits_n(word, n):\n",
    "    es = set([word])\n",
    "    for i in range(n):\n",
    "        es = reduce(lambda a, b: a.union(b), (edits(w) for w in es))\n",
    "    return es\n",
    "\n",
    "def words(text):\n",
    "    return text.split()\n",
    "\n",
    "def known_words(words):\n",
    "    return {word for word in words if word in WORDS}\n",
    "\n",
    "def candidate_words(word):\n",
    "    return (known_words([word]) or known_words(edits_n(word, 1)) or known_words(edits_n(word, 2)) or [word])\n",
    "\n",
    "list(candidate_words(\"swam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/lm/words.txt') as f:\n",
    "    WORDS = set(words(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r:a ring of amethyst i could not wear here plainer to my sight than that first kiss\n",
    "p:a ring of amathyst i could not wear here plainer two my sight then that first kits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she doesn't take up with anybody you know\n",
      "she doesn't take up with anybody you know\n",
      "langauge model changed prediction, WER changed from 0.0243902439024 to 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"she doesn't take up with anybody you know\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lm_alternative(res[3][3], res[2][3], zip(*beam_decode_u(res[0][:, 3, :], 12, normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sir i have it in command to inform your excellency that you have been appointed governor of the crown colony which is called britannula\n",
      "sir i have in command to anform your excellency that you have been appointed governor of the crown colony which is called britain mula\n",
      "langauge model changed prediction, WER changed from 0.0334572490706 to 0.0334572490706\n",
      "sir i have in command to anform your excellency that you have been appointed governor of the crown colony which is called britain mula\n",
      "sir i have in command to anform your excellency that you have been appointed governor of the crown colony which is called britaan mula\n"
     ]
    }
   ],
   "source": [
    "print best_lm_alternative(res[3][46], res[2][46], zip(*beam_decode_u(res[0][:, 46, :], 12, normalize=False)))\n",
    "print res[1][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16216216216216217"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import edit_distance\n",
    "ref = 'there is no danger of the modern commentators on the timaeus falling into the absurdities of the neo platonists'\n",
    "pre = 'there is old danger of the madern commontychers un ther to meas falling into dubsurdities of the newo platinists'\n",
    "pre = 'there is old danger of the madern commontychers un ther to mes falling into dubsurdities of the newo platinists'\n",
    "#print edit_distance.SequenceMatcher(ref, pre).ratio()\n",
    "word_error_rate([ref], [pre], decoded=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\"duration\": 4.905, \"text\": \"he began a confused complaint against the wizard who had vanished behind the curtain on the left\", \"key\": \"/mnt/ml-data/LibriSpeech/test-clean/61/70968/61-70968-0000.wav\"},\n",
    "    {\"duration\": 3.61, \"text\": \"give not so earnest a mind to these mummeries child\", \"key\": \"/mnt/ml-data/LibriSpeech/test-clean/61/70968/61-70968-0001.wav\"}    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(test_generator(datagen, samples, normalize=True), output_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
